{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 12,
         "id": "9a164089",
         "metadata": {},
         "outputs": [],
         "source": [
            "# PyTorch\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.optim as optim\n",
            "import torch.nn.functional as F # FFFFF\n",
            "\n",
            "# Data loading\n",
            "import torchvision.datasets as datasets\n",
            "import torchvision.transforms as transforms\n",
            "from torch.utils.data import DataLoader, random_split\n",
            "from torch.utils.data.sampler import SubsetRandomSampler\n",
            "\n",
            "# Auxiliary functions\n",
            "from torch.utils.tensorboard import SummaryWriter  # Used for Tensorboard logging\n",
            "import os\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "from math import floor, ceil\n",
            "import datetime\n",
            "\n",
            "import json\n",
            "import dataset_semseg"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "f31ddcf1",
         "metadata": {},
         "source": [
            "# Carga de dataloaders baja resolucion, 10 clases"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "c7b08ce0",
         "metadata": {},
         "source": [
            "Leemos las imagenes y sus mascaras con el objeto proporcionado"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "62274141",
         "metadata": {},
         "outputs": [],
         "source": [
            "train=dataset_semseg.SupermarketSemSeg(\"dataset_res_144_192_10classes/train/images\",\"dataset_res_144_192_10classes/train/masks\")\n",
            "test_dataset=dataset_semseg.SupermarketSemSeg(\"dataset_res_144_192_10classes/test/images\",\"dataset_res_144_192_10classes/test/masks\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "bf7286fe",
         "metadata": {},
         "source": [
            "Separamos train en train/validacion 80/20.Con semilla para obtener siempre la misma particion"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "0fa7fbd5",
         "metadata": {},
         "outputs": [],
         "source": [
            "total_size = len(train)\n",
            "train_size = int(0.8 * total_size)\n",
            "val_size = total_size - train_size\n",
            "\n",
            "generator = torch.Generator().manual_seed(42)\n",
            "\n",
            "train_dataset, val_dataset = random_split(\n",
            "    train, \n",
            "    [train_size, val_size], \n",
            "    generator=generator\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "69232daf",
         "metadata": {},
         "source": [
            "Creamos los dataloaders"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "fb302e18",
         "metadata": {},
         "outputs": [],
         "source": [
            "batch_size = 35  # Tamaño de lotes\n",
            "num_workers = 0  # Controla cuántos procesos cargan datos en paralelo (lo dejaremos a 0 para ahorrar recursos)\n",
            "\n",
            "train_loader = DataLoader(\n",
            "    dataset=train_dataset,      \n",
            "    batch_size=batch_size,\n",
            "    shuffle=True,\n",
            "    num_workers=num_workers\n",
            ")\n",
            "\n",
            "val_loader = DataLoader(\n",
            "    dataset=val_dataset,        \n",
            "    batch_size=batch_size,\n",
            "    shuffle=True,\n",
            "    num_workers=num_workers\n",
            ")\n",
            "\n",
            "test_loader = DataLoader(\n",
            "    dataset=test_dataset,\n",
            "    batch_size=batch_size,\n",
            "    shuffle=False,\n",
            "    num_workers=num_workers\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "0701c2fd",
         "metadata": {},
         "source": [
            "# Segmentación de imagen mediante arquitectura U-Net"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "id": "feeee349",
         "metadata": {},
         "outputs": [],
         "source": [
            "PATH_ROOT = os.path.join('.')\n",
            "# Ruta para datos:\n",
            "PATH_DATA = os.path.join(PATH_ROOT, 'data')\n",
            "# Ruta para modelos:\n",
            "PATH_MODELS = os.path.join(PATH_ROOT, 'reports', 'models')\n",
            "# Ruta para resultados:\n",
            "PATH_RESULTS = os.path.join(PATH_ROOT, 'reports', 'results')\n",
            "# Ruta para ejecuciones:\n",
            "PATH_RUNS = os.path.join(PATH_ROOT, 'reports', 'runs')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "id": "f126c56b",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Nombre del directorio de pruebas: 2025_11_19__10_0\n"
               ]
            }
         ],
         "source": [
            "# Para cada sesión creamos un directorio nuevo, a partir de la fecha y hora de su ejecución:\n",
            "date = datetime.datetime.now()\n",
            "test_name = str(date.year) + '_' + str(date.month) + '_' +  str(date.day) + '__' + str(date.hour) + '_' + str(date.minute)\n",
            "print('Nombre del directorio de pruebas: {}'.format(test_name))\n",
            "models_folder = os.path.join(PATH_MODELS, test_name)\n",
            "try:\n",
            "    os.makedirs(models_folder)\n",
            "except:\n",
            "    print(f'Folder {models_folder} already existed.')\n",
            "results_folder = os.path.join(PATH_RESULTS, test_name)\n",
            "try:\n",
            "    os.makedirs(results_folder)\n",
            "except:\n",
            "    print(f'Folder {results_folder} already existed.')\n",
            "runs_folder = os.path.join(PATH_RUNS, test_name)\n",
            "try:\n",
            "    os.makedirs(runs_folder)\n",
            "except:\n",
            "    print(f'Folder {runs_folder} already existed.')\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "id": "d06242d0",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "cuda\n"
               ]
            }
         ],
         "source": [
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "print(device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 22,
         "id": "ad92db39",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{0: 'background',\n",
                     " 1: 'coca_cola_05',\n",
                     " 2: 'koelln_muesli_schoko',\n",
                     " 3: 'apple_granny_smith',\n",
                     " 4: 'banana_single',\n",
                     " 5: 'oranges',\n",
                     " 6: 'pasta_reggia_spaghetti',\n",
                     " 7: 'gepa_bio_und_fair_fencheltee',\n",
                     " 8: 'cucumber',\n",
                     " 9: 'carrot',\n",
                     " 10: 'lettuce'}"
                  ]
               },
               "execution_count": 22,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "def leer_mapeado(ruta):\n",
            "    with open(ruta, 'r') as f:\n",
            "        data = json.load(f)\n",
            "    clases = {}\n",
            "    for i in range(len(data)):\n",
            "        clases[data[i]['index']] = data[i]['name']\n",
            "    return clases\n",
            "leer_mapeado('dataset_res_144_192_10classes/label_mapping_10classes.json')"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "ad2326a9",
         "metadata": {},
         "source": [
            "## Definición del modelo"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "2bcb3d85",
         "metadata": {},
         "source": [
            "Como vamos a repetir la estructura de Convolución, Convolución, pooling, hacemos una clase para ello"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "id": "b0a166a3",
         "metadata": {},
         "outputs": [],
         "source": [
            "class dobleConvolucionMaxPool(nn.Module):\n",
            "    def __init__(self, in_channels, out_channels):\n",
            "        super().__init__()\n",
            "        self.convolucion_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)  \n",
            "        self.convolucion_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
            "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
            "    \n",
            "    def forward(self, x):\n",
            "        x = torch.nn.functional.relu(self.convolucion_1(x))\n",
            "        x = torch.nn.functional.relu(self.convolucion_2(x))\n",
            "        skip_connection = x\n",
            "        x = self.pool(x)\n",
            "        return x, skip_connection"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "75c67df8",
         "metadata": {},
         "source": [
            "Lo mismo para hacer las convoluciones y deconvolución"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "id": "0ef5491d",
         "metadata": {},
         "outputs": [],
         "source": [
            "class deconvolucionDobleConvolucion(nn.Module):\n",
            "    def __init__(self, in_channels, out_channels):\n",
            "        super().__init__()\n",
            "        self.deconvolucion = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
            "        self.convolucion_1 = nn.Conv2d(out_channels * 2, out_channels, kernel_size=3, padding=1)  # *2 porque concatenamos\n",
            "        self.convolucion_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
            "        \n",
            "    \n",
            "    def forward(self, x, skip_connection):\n",
            "        x = self.deconvolucion(x)\n",
            "        x = torch.nn.functional.relu(self.convolucion_1(torch.cat([x, skip_connection], dim=1)))\n",
            "        x = torch.nn.functional.relu(self.convolucion_2(x))\n",
            "        return x"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "id": "f0e107bf",
         "metadata": {},
         "outputs": [],
         "source": [
            "def calcular_num_filtros(num_base_filtros, num_niveles):\n",
            "    filtros = []\n",
            "    for i in range(num_niveles):\n",
            "        filtros.append(num_base_filtros * (2**i)) # porque en cada nivel quermos duplicar el num de filtros\n",
            "    return filtros"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "14695004",
         "metadata": {},
         "outputs": [],
         "source": [
            "class MLP(nn.Module):\n",
            "    def __init__(self, in_dim, hidden_sizes=[10, 10], bias=True):\n",
            "        super().__init__()\n",
            "        self.capa_1 = nn.Linear(in_dim, hidden_sizes[0], bias=bias, device=device)\n",
            "        self.capa_2 = nn.Linear(hidden_sizes[0], hidden_sizes[1], bias=bias, device=device)\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = torch.nn.functional.relu(self.capa_1(x))\n",
            "        x = self.capa_2(x) \n",
            "        return x"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "cddadc6f",
         "metadata": {},
         "outputs": [],
         "source": [
            "class UnetModel(nn.Module):\n",
            "\n",
            "    def __init__(self, num_base_filtros = 64, num_niveles = 4, num_clases = 10):\n",
            "        super().__init__()\n",
            "        self.encoders = nn.ModuleList()\n",
            "        self.decoders = nn.ModuleList()\n",
            "        self.filtros = calcular_num_filtros(num_base_filtros, num_niveles + 1)  # [64, 128, 256, 512, 1024] por defecto. Podemos aumentar el número de niveles para comprobar rendimiento\n",
            "        for i in range(num_niveles):\n",
            "            self.encoders.append(dobleConvolucionMaxPool(in_channels=self.filtros[i], out_channels=self.filtros[i+1]))\n",
            "            self.decoders.append(deconvolucionDobleConvolucion(in_channels=self.filtros[num_niveles-i], out_channels=self.filtros[num_niveles-1-i])) # Queremos ir al revés\n",
            "        self.cuello_botella = nn.Conv2d(self.filtros[-1], self.filtros[-1], kernel_size=3, padding=1)\n",
            "        #self.cuello_botella = MLP(self.filtros[-1], [4096, 4096]) # IMPORTANTE: preguntar si usar modelo de diapositivas o del pdf (lo mismo para el unpooling/deconvolución)\n",
            "        self.convolucion_final = nn.Conv2d(self.filtros[0], num_clases+1, kernel_size=1) # +1 por el background\n",
            "    \n",
            "    def forward(self, X):\n",
            "        skip_connections = []\n",
            "        for encoder in self.encoders:\n",
            "            X, skip_connection = encoder(X)\n",
            "            skip_connections.append(skip_connection)\n",
            "        #X_shape = X.shape\n",
            "        #X = X.flatten(1)\n",
            "        X = self.cuello_botella(X)\n",
            "        #X = X.reshape(X_shape)\n",
            "        for i, decoder in enumerate(self.decoders):\n",
            "            X = decoder(X, skip_connections[len(skip_connections)-1-i])\n",
            "        X = self.convolucion_final(X)\n",
            "        return X"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "a3d00d59",
         "metadata": {},
         "source": [
            "# Entrenamiento del modelo"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "61acc2c6",
         "metadata": {},
         "source": [
            "Durante el entrenamiento, para saber si este ha sido fructuoso, registraremos las siguientes métricas\n",
            "\n",
            "- Salida del modelo para algunas imágenes\n",
            "- Loss durante el entrenamiento\n",
            "- Accuracy durante el entrenamiento"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "7814e73c",
         "metadata": {},
         "outputs": [],
         "source": [
            "def train(model, train_loader, criterion, optimizer, summary_writer, val_loader=None, num_epochs=20, device='cuda'):\n",
            "\n",
            "    # Listas para generar logs durante el entrenamiento:\n",
            "    train_acc = []  \n",
            "    train_loss = []\n",
            "    if val_loader is not None:\n",
            "        val_acc = []\n",
            "        val_loss = []\n",
            "\n",
            "    # Bucle de entrenamiento:\n",
            "    for epoch in range(num_epochs):\n",
            "        running_loss = 0.0  # Acumulamos el valor de coste obtenido tras cada epoch\n",
            "        count_evaluated = 0\n",
            "        count_correct = 0\n",
            "        for batch_idx, data in enumerate(train_loader, 0):    \n",
            "            model.train()  \n",
            "            inputs, labels = data[0].to(device), data[1].to(device)  \n",
            "            optimizer.zero_grad()\n",
            "            imgs, masks = inputs \n",
            "            imagen_filtrada = torch.where(masks > 0, imgs, 0)\n",
            "            \n",
            "            outputs = model(inputs)\n",
            "            loss = criterion(outputs, labels)\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "            ### Fase de log ###\n",
            "            running_loss += loss.item()  # Acumulamos el error obtenido para utilizarlo\n",
            "                # a la hora de generar logs del proceso.\n",
            "            # Contamos el número de ejemplos evaluados y acertados:\n",
            "            count_evaluated += inputs.shape[0]\n",
            "            count_correct += torch.sum(labels == torch.max(outputs, dim=1)[1])\n",
            "        if outputs: # Guardamos la predicción de un batch cada epoch\n",
            "            summary_writer.add_histogram(\"predicciones\", outputs.cpu(), epoch)\n",
            "        # Log del valor de la función de coste y accuracy\n",
            "        print('Training: [%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / (batch_idx+1)))\n",
            "        train_loss.append(running_loss / (batch_idx+1))\n",
            "        # Almacenamos la accuracy al final de la epoch (en train)\n",
            "        train_acc.append(float(count_correct) / count_evaluated)\n",
            "\n",
            "        summary_writer.add_scalar(\"coste_train\", train_loss[-1], epoch)\n",
            "        summary_writer.add_scalar(\"acc_train\", train_acc[-1], epoch)\n",
            "        \n",
            "        ### Fase de validación ### \n",
            "        if val_loader is not None:\n",
            "            running_loss_val = 0.0\n",
            "            count_evaluated = 0\n",
            "            count_correct = 0\n",
            "            model.eval()\n",
            "            with torch.no_grad():\n",
            "                for val_batch_idx, data_val in enumerate(val_loader, 0):\n",
            "                    inputs_val, labels_val = data_val[0].to(device), data_val[1].to(device)\n",
            "                    outputs_val = model(inputs_val)\n",
            "                    loss = criterion(outputs_val, labels_val)\n",
            "                    running_loss_val += loss.item()\n",
            "                    count_evaluated += inputs_val.shape[0]\n",
            "                    count_correct += torch.sum(labels_val == torch.max(outputs_val, dim=1)[1])\n",
            "                # Presentamos el resumen de la validación de la epoch:\n",
            "                val_loss.append(running_loss_val / (val_batch_idx + 1))\n",
            "                acc_val = float(count_correct) / count_evaluated\n",
            "                \n",
            "                summary_writer.add_scalar(\"acc_val\", acc_val, epoch)\n",
            "                print('Validation: epoch %d - acc: %.3f' %\n",
            "                            (epoch + 1, acc_val))\n",
            "                val_acc.append(acc_val)\n",
            "        for name, param in model.named_parameters():\n",
            "            summary_writer.add_histogram(f\"param_{name}\", param, epoch)\n",
            "            if param.grad is not None:\n",
            "                summary_writer.add_histogram(f\"grad_{name}\", param.grad, epoch)\n",
            "\n",
            "    # Devolvemos, tanto el modelo entrenado, como el diccionario con las estadísticas del entrenamiento\n",
            "    return model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "4b2b4c53",
         "metadata": {},
         "outputs": [],
         "source": [
            "def test(model, test_loader, criterion, device='cuda'):\n",
            "    with torch.no_grad():\n",
            "        number_samples = 0\n",
            "        number_correct = 0\n",
            "        running_loss_test = 0.0\n",
            "        for test_batch_idx, data_test in enumerate(test_loader, 0):\n",
            "            inputs_test, labels_test = data_test[0].to(device), data_test[1].long().to(device)\n",
            "            imgs, masks = inputs_test\n",
            "            outputs_test = model(inputs_test)\n",
            "            loss = criterion(outputs_test, labels_test)\n",
            "            running_loss_test += loss.cpu().numpy()\n",
            "            # Accuracy:\n",
            "            _, outputs_class = torch.max(outputs_test, dim=1)\n",
            "            number_correct += torch.sum(outputs_class == labels_test).cpu().numpy()\n",
            "            number_samples += len(labels_test)\n",
            "        acc_test = number_correct / number_samples\n",
            "        print('Test - Accuracy: %.3f' % acc_test)\n",
            "        print('Test - CrossEntropy: %.3f' % (running_loss_test / (test_batch_idx+1)))"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "4313f453",
         "metadata": {},
         "source": [
            "Creamos el modelo, usamos un optimizador arbitrario y lo entrenamos para el dataset con 10 clases."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "401b6008",
         "metadata": {},
         "outputs": [],
         "source": [
            "model = UnetModel(num_base_filtros=64, num_clases=10, num_niveles=4)\n",
            "criterion = torch.nn.CrossEntropyLoss()\n",
            "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0001)\n",
            "writer = torch.utils.tensorboard.SummaryWriter(log_dir=PATH_RUNS)\n",
            "model = train(model, train_loader, criterion, optimizer, writer, val_loader = val_loader, device=device)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "0f1f2b0f",
         "metadata": {},
         "source": [
            "## Funcion de coste"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "5773856e",
         "metadata": {},
         "source": [
            "## Optimizador"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "d1346ccd",
         "metadata": {},
         "source": [
            "### Evolución de factor de aprendizaje"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "deep",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.19"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}