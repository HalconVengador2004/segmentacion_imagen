{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a164089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F # FFFFF\n",
    "\n",
    "# Data loading\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Auxiliary functions\n",
    "from torch.utils.tensorboard import SummaryWriter  # Used for Tensorboard logging\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import floor, ceil\n",
    "import datetime\n",
    "import dataset_semseg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ddcf1",
   "metadata": {},
   "source": [
    "# Carga de dataloaders baja resolucion, 10 clases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b08ce0",
   "metadata": {},
   "source": [
    "Leemos las imagenes y sus mascaras con el objeto proporcionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62274141",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=dataset_semseg.SupermarketSemSeg(\"dataset_res_144_192_10classes/train/images\",\"dataset_res_144_192_10classes/train/masks\")\n",
    "test_dataset=dataset_semseg.SupermarketSemSeg(\"dataset_res_144_192_10classes/test/images\",\"dataset_res_144_192_10classes/test/masks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7286fe",
   "metadata": {},
   "source": [
    "Separamos train en train/validacion 80/20.Con semilla para obtener siempre la misma particion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa7fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(train)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    train, \n",
    "    [train_size, val_size], \n",
    "    generator=generator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69232daf",
   "metadata": {},
   "source": [
    "Creamos los dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb302e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 35  # Tamaño de lotes\n",
    "num_workers = 0  # Controla cuántos procesos cargan datos en paralelo (lo dejaremos a 0 para ahorrar recursos)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,      \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,        \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0701c2fd",
   "metadata": {},
   "source": [
    "# Segmentación de imagen mediante arquitectura U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feeee349",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ROOT = os.path.join('.')\n",
    "# Ruta para datos:\n",
    "PATH_DATA = os.path.join(PATH_ROOT, 'data')\n",
    "# Ruta para modelos:\n",
    "PATH_MODELS = os.path.join(PATH_ROOT, 'reports', 'models')\n",
    "# Ruta para resultados:\n",
    "PATH_RESULTS = os.path.join(PATH_ROOT, 'reports', 'results')\n",
    "# Ruta para ejecuciones:\n",
    "PATH_RUNS = os.path.join(PATH_ROOT, 'reports', 'runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f126c56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre del directorio de pruebas: 2025_11_12__21_25\n"
     ]
    }
   ],
   "source": [
    "# Para cada sesión creamos un directorio nuevo, a partir de la fecha y hora de su ejecución:\n",
    "date = datetime.datetime.now()\n",
    "test_name = str(date.year) + '_' + str(date.month) + '_' +  str(date.day) + '__' + str(date.hour) + '_' + str(date.minute)\n",
    "print('Nombre del directorio de pruebas: {}'.format(test_name))\n",
    "models_folder = os.path.join(PATH_MODELS, test_name)\n",
    "try:\n",
    "    os.makedirs(models_folder)\n",
    "except:\n",
    "    print(f'Folder {models_folder} already existed.')\n",
    "results_folder = os.path.join(PATH_RESULTS, test_name)\n",
    "try:\n",
    "    os.makedirs(results_folder)\n",
    "except:\n",
    "    print(f'Folder {results_folder} already existed.')\n",
    "runs_folder = os.path.join(PATH_RUNS, test_name)\n",
    "try:\n",
    "    os.makedirs(runs_folder)\n",
    "except:\n",
    "    print(f'Folder {runs_folder} already existed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d06242d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2326a9",
   "metadata": {},
   "source": [
    "## Definición del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcb3d85",
   "metadata": {},
   "source": [
    "Como vamos a repetir la estructura de Convolución, Convolución, pooling, hacemos una clase para ello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a166a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dobleConvolucionMaxPool(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.convolucion_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)  \n",
    "        self.convolucion_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.convolucion_1(x))\n",
    "        x = torch.nn.functional.relu(self.convolucion_2(x))\n",
    "        skip_connection = x\n",
    "        x = self.pool(x)\n",
    "        return x, skip_connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c67df8",
   "metadata": {},
   "source": [
    "Lo mismo para hacer las convoluciones y deconvolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef5491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class deconvolucionDobleConvolucion(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.deconvolucion = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.convolucion_1 = nn.Conv2d(out_channels * 2, out_channels, kernel_size=3, padding=1)  # *2 porque concatenamos\n",
    "        self.convolucion_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, skip_connection):\n",
    "        x = self.deconvolucion(x)\n",
    "        x = torch.nn.functional.relu(self.convolucion_1(torch.cat([x, skip_connection], dim=1)))\n",
    "        x = torch.nn.functional.relu(self.convolucion_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e107bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_num_filtros(num_base_filtros, num_niveles):\n",
    "    filtros = []\n",
    "    for i in range(num_niveles):\n",
    "        filtros.append(num_base_filtros * (2**i)) # porque en cada nivel quermos duplicar el num de filtros\n",
    "    return filtros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14695004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_sizes=[10, 10], bias=True):\n",
    "        super().__init__()\n",
    "        self.capa_1 = nn.Linear(in_dim, hidden_sizes[0], bias=bias, device=device)\n",
    "        self.capa_2 = nn.Linear(hidden_sizes[0], hidden_sizes[1], bias=bias, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.capa_1(x))\n",
    "        x = self.capa_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddadc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_base_filtros = 64, num_niveles = 4, num_clases = 10):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.decoders = nn.ModuleList()\n",
    "        self.filtros = calcular_num_filtros(num_base_filtros, num_niveles + 1)  # [64, 128, 256, 512, 1024] por defecto. Podemos aumentar el número de niveles para comprobar rendimiento\n",
    "        for i in range(num_niveles):\n",
    "            self.encoders.append(dobleConvolucionMaxPool(in_channels=self.filtros[i], out_channels=self.filtros[i+1]))\n",
    "            self.decoders.append(deconvolucionDobleConvolucion(in_channels=self.filtros[num_niveles-i], out_channels=self.filtros[num_niveles-1-i])) # Queremos ir al revés\n",
    "        self.cuello_botella = MLP(self.filtros[-1], [4096, 4096]) # IMPORTANTE: preguntar si usar modelo de diapositivas o del pdf (lo mismo para el unpooling/deconvolución)\n",
    "        self.convolucion_final = nn.Conv2d(self.filtros[0], num_clases, kernel_size=1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        skip_connections = []\n",
    "        for encoder in self.encoders:\n",
    "            X, skip_connection = encoder(X)\n",
    "            skip_connections.append(skip_connection)\n",
    "        X_shape = X.shape\n",
    "        X = X.flatten(1)\n",
    "        X = self.cuello_botella(X)\n",
    "        X = X.reshape(X_shape)\n",
    "        for i, decoder in enumerate(self.decoders):\n",
    "            X = decoder(X, skip_connections[len(skip_connections)-1-i])\n",
    "        X = self.convolucion_final(X)\n",
    "        return X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
